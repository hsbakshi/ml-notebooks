{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "authorship_tag": "ABX9TyPVXoqnPuflB0kcAkpOTcuL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsbakshi/ml-notebooks/blob/main/whisper_diarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qq https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip\n",
        "from pyannote.audio import Audio\n"
      ],
      "metadata": {
        "id": "kFf4HhNmTkJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "%pip install -qq cog\n",
        "%pip install -qq faster_whisper\n"
      ],
      "metadata": {
        "id": "fzk7hd9ym5ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cog import BasePredictor, Input, Path, BaseModel\n",
        "import os\n",
        "import time\n",
        "import wave\n",
        "import torch\n",
        "from faster_whisper import WhisperModel\n",
        "import datetime\n",
        "import contextlib\n",
        "import numpy as np\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "from typing import Any\n",
        "from sklearn.metrics import silhouette_score\n"
      ],
      "metadata": {
        "id": "6nYPQkjCyOBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKOk-xAemiBo"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ModelOutput(BaseModel):\n",
        "    segments: Any\n",
        "\n",
        "class Predictor(BasePredictor):\n",
        "    def setup(self):\n",
        "        model_name = \"medium\"\n",
        "        self.model = WhisperModel(model_name, device=\"cuda\", compute_type=\"float16\")\n",
        "        self.embedding_model = PretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "                                                          device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    def predict(self, audio: Path = Input(description=\"An audio file\", default=None),\n",
        "                group_segments: bool = Input(description=\"Group segments of the same speaker shorter apart than 2 seconds\", default=True),\n",
        "                num_speakers: int = Input(description=\"Number of speakers\", ge=0, le=25, default=0),\n",
        "                prompt: str = Input(description=\"Prompt, to be used as context\", default=\"Some people speaking.\"),\n",
        "                offset_seconds: int = Input(description=\"Offset in seconds for chunking inputs\", default=0, ge=0)) -> ModelOutput:\n",
        "        filepath = audio\n",
        "        segments = self.speech_to_text(filepath, num_speakers, prompt, offset_seconds, group_segments)\n",
        "        return ModelOutput(segments=segments, offset_seconds=offset_seconds)\n",
        "\n",
        "    def convert_time(self, secs, offset_seconds=0):\n",
        "        return datetime.timedelta(seconds=(round(secs) + offset_seconds))\n",
        "\n",
        "    def speech_to_text(self, filepath, num_speakers, prompt=\"People talking.\", offset_seconds=0, group_segments=True):\n",
        "        time_start = time.time()\n",
        "        try:\n",
        "            audio_file_wav = self.convert_audio_to_wav(filepath)\n",
        "            duration = self.get_audio_duration(audio_file_wav)\n",
        "            segments = self.transcribe_audio(audio_file_wav, prompt)\n",
        "            segments = self.convert_segments(segments)\n",
        "            embeddings = self.create_embeddings(segments, audio_file_wav, duration)\n",
        "            speaker_count = self.find_speaker_count(embeddings, num_speakers)\n",
        "            output = self.assign_speaker_labels(segments, embeddings, speaker_count, offset_seconds, group_segments)\n",
        "            time_end = time.time()\n",
        "            time_diff = time_end - time_start\n",
        "            system_info = f\"Processing time: {time_diff:.5} seconds\"\n",
        "            print(system_info)\n",
        "            os.remove(audio_file_wav)\n",
        "            return output\n",
        "        except Exception as e:\n",
        "            os.remove(audio_file_wav)\n",
        "            raise RuntimeError(\"Error running inference with local model\", e)\n",
        "\n",
        "    def convert_audio_to_wav(self, filepath):\n",
        "        file_ending = os.path.splitext(f'{filepath}')[-1]\n",
        "        print(f'File ending: \"{file_ending}\"')\n",
        "        if file_ending != '.wav':\n",
        "            audio_file_wav = str(filepath).replace(file_ending, \".wav\")\n",
        "            print(\"Starting conversion to wav\")\n",
        "            os.system(f'ffmpeg -i \"{filepath}\" -ar 16000 -ac 1 -c:a pcm_s16le \"{audio_file_wav}\"')\n",
        "        else:\n",
        "            audio_file_wav = filepath\n",
        "        return audio_file_wav\n",
        "\n",
        "    def get_audio_duration(self, audio_file_wav):\n",
        "        with contextlib.closing(wave.open(audio_file_wav, 'r')) as f:\n",
        "            frames = f.getnframes()\n",
        "            rate = f.getframerate()\n",
        "            duration = frames / float(rate)\n",
        "        print(f\"Conversion to wav ready, duration of audio file: {duration}\")\n",
        "        return duration\n",
        "\n",
        "    def transcribe_audio(self, audio_file_wav, prompt):\n",
        "        print(\"Starting whisper\")\n",
        "        options = dict(beam_size=5, best_of=5)\n",
        "        transcribe_options = dict(task=\"transcribe\",\n",
        "                                  word_timestamps=True,\n",
        "                                  vad_filter=True,\n",
        "                                  **options)\n",
        "        print(prompt)\n",
        "        segments, _ = self.model.transcribe(audio_file_wav, **transcribe_options, initial_prompt=prompt)\n",
        "        print(\"Done with whisper\")\n",
        "        result = list(segments)\n",
        "        print(f\"Sample segment::{result[0]}\")\n",
        "        return result\n",
        "\n",
        "    def convert_segments(self, segments):\n",
        "        return [\n",
        "            {\n",
        "                'start': int(s.start),\n",
        "                'end': int(s.end),\n",
        "                'text': s.text,\n",
        "                'words': s.words\n",
        "            }\n",
        "            for s in segments]\n",
        "\n",
        "    def create_embeddings(self, segments, audio_file_wav, duration):\n",
        "        print(\"Starting embedding\")\n",
        "        embeddings = np.zeros(shape=(len(segments), 192))\n",
        "        audio = Audio()\n",
        "        for i, segment in enumerate(segments):\n",
        "            waveform, sample_rate = audio.crop(audio_file_wav, Segment(segment[\"start\"], min(duration, segment[\"end\"])))\n",
        "            embeddings[i] = self.embedding_model(waveform[None])\n",
        "        embeddings = np.nan_to_num(embeddings)\n",
        "        print(f'Embedding shape: {embeddings.shape}')\n",
        "        return embeddings\n",
        "    \n",
        "    def find_speaker_count(self, embeddings, speaker_count_override):\n",
        "        if speaker_count_override == 0:\n",
        "            # Find the best number of speakers\n",
        "            score_num_speakers = {}\n",
        "            for num_speakers in range(2, 10):\n",
        "                clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "                score = silhouette_score(embeddings, clustering.labels_, metric='euclidean')\n",
        "                score_num_speakers[num_speakers] = score\n",
        "            best_num_speaker = max(score_num_speakers, key=lambda x:score_num_speakers[x])\n",
        "            print(f\"The best number of speakers: {best_num_speaker} with {score_num_speakers[best_num_speaker]} score\")\n",
        "        else:\n",
        "            best_num_speaker = speaker_count_override\n",
        "        return best_num_speaker\n",
        "\n",
        "    def assign_speaker_labels(self, segments, embeddings, num_speakers, offset_seconds, group_segments):\n",
        "        clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "        labels = clustering.labels_\n",
        "        for i in range(len(segments)):\n",
        "            segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "        output = []\n",
        "        current_group = {\n",
        "            'start': str(round(segments[0][\"start\"] + offset_seconds)),\n",
        "            'end': str(round(segments[0][\"end\"] + offset_seconds)),\n",
        "            'speaker': segments[0][\"speaker\"],\n",
        "            'text': segments[0][\"text\"],\n",
        "            'words': segments[0][\"words\"]\n",
        "        }\n",
        "        for i in range(1, len(segments)):\n",
        "            time_gap = segments[i][\"start\"] - segments[i - 1][\"end\"]\n",
        "            if segments[i][\"speaker\"] == segments[i - 1][\"speaker\"] and time_gap <= 2 and group_segments:\n",
        "                current_group[\"end\"] = str(round(segments[i][\"end\"] + offset_seconds))\n",
        "                current_group[\"text\"] += \" \" + segments[i][\"text\"]\n",
        "            else:\n",
        "                output.append(current_group)\n",
        "                current_group = {\n",
        "                    'start': str(round(segments[i][\"start\"] + offset_seconds)),\n",
        "                    'end': str(round(segments[i][\"end\"] + offset_seconds)),\n",
        "                    'speaker': segments[i][\"speaker\"],\n",
        "                    'text': segments[i][\"text\"],\n",
        "                    'words': segments[i][\"words\"]\n",
        "                }\n",
        "        output.append(current_group)\n",
        "        print(\"Embedding complete\")\n",
        "        return output\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ZAAorYMdXISr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "fUfgm_iDXI3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the Predictor class"
      ],
      "metadata": {
        "id": "DXClwpVCWJWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = Predictor()\n",
        "predictor.setup()"
      ],
      "metadata": {
        "id": "3kbKIMwjUxLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the test case"
      ],
      "metadata": {
        "id": "AX-3AVKGWOfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.colab\n",
        "own_file, _ = google.colab.files.upload().popitem()\n",
        "OWN_FILE = {'audio': own_file}\n",
        "\n"
      ],
      "metadata": {
        "id": "OQDvcII3WIQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.predict(OWN_FILE['audio'], group_segments=False, num_speakers=0, prompt=\"People talking\", offset_seconds=0)"
      ],
      "metadata": {
        "id": "NsaUaRElbxMa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}